{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6d97f7-60cf-4f22-b5c6-25a8d04848e1",
   "metadata": {},
   "source": [
    "# AutoTagger – Smart Tags & Topic Extractor\n",
    "\n",
    "This project demonstrates how to extract meaningful **tags** and discover **dominant topics** from a long-form article using Natural Language Processing techniques.\n",
    "\n",
    "### Objectives:\n",
    "- Extract **domain-relevant tags** using TF-IDF and linguistic filtering\n",
    "- Detect **latent topics** using unsupervised topic modeling (LDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68483ae7-2f9d-4311-9d43-cd3e1bf92b87",
   "metadata": {},
   "source": [
    "## Install and Import Required Libraries\n",
    "\n",
    "- `nltk` for stopword handling\n",
    "- `spaCy` for part-of-speech tagging\n",
    "- `scikit-learn` for TF-IDF vectorization\n",
    "- `gensim` for LDA topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbae95b1-5b80-47e6-98d7-6dacd60939ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JeevanaSree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 3.9/12.8 MB 23.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.6/12.8 MB 17.5 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 16.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.1 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install once (if needed)\n",
    "!pip install nltk scikit-learn gensim spacy pyLDAvis  --quiet\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcebbbe-9f18-48fe-8ed2-6191c8f42b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffef415-d1f8-4529-bfc8-4ecc5f4206c0",
   "metadata": {},
   "source": [
    "## Input Sample Article\n",
    "\n",
    "This text simulates a real-world blog/article about **Artificial Intelligence in Healthcare**. It will serve as our base document for tag and topic extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d04c182-2b74-415b-bdf6-7966b2ec52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Artificial Intelligence (AI) is playing an increasingly important role in healthcare, particularly in the domains of diagnostics and treatment personalization.\n",
    "With the explosion of digital medical records and health data, AI is being used to detect patterns and make predictions that assist doctors and researchers alike.\n",
    "Machine learning algorithms can now analyze X-rays, MRIs, and other medical imaging data faster and in some cases more accurately than human radiologists.\n",
    "AI tools are also being integrated into electronic health records (EHRs) to automate administrative tasks, allowing clinicians to focus more on patient care.\n",
    "Natural Language Processing (NLP) helps in structuring unstructured data from clinical notes, medical literature, and patient interaction transcripts.\n",
    "\n",
    "In surgery, robotic systems powered by AI are increasing precision, reducing complications, and improving recovery times.\n",
    "Remote surgeries using robotic arms, guided by AI-driven feedback mechanisms, are becoming a reality in rural and underserved areas.\n",
    "Predictive analytics using AI models can forecast outbreaks, anticipate patient deterioration, and optimize resource allocation in hospitals.\n",
    "\n",
    "However, the integration of AI in healthcare is not without challenges.\n",
    "Issues such as data privacy, algorithmic bias, interpretability of models, and the need for regulatory compliance remain key concerns.\n",
    "There is also a need for continuous training of AI models with diverse datasets to ensure fairness and accuracy in predictions.\n",
    "\n",
    "Despite these challenges, the future of AI in healthcare looks promising.\n",
    "With advancements in deep learning, reinforcement learning, and data integration, AI is set to revolutionize not just diagnostics and treatment but the entire healthcare ecosystem.\n",
    "Researchers and practitioners must work collaboratively to harness AI’s full potential while maintaining ethical and human-centered practices.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524d888-91f1-4681-8e7b-caf6333796d3",
   "metadata": {},
   "source": [
    "## Preprocess and Extract Tags using TF-IDF + POS Filtering\n",
    "\n",
    "Why this approach?\n",
    "- TF-IDF gives high importance to unique and rare words in context\n",
    "- However, raw TF-IDF often includes irrelevant or noisy terms (verbs, adverbs)\n",
    "- So we filter the top TF-IDF words to **nouns and proper nouns** using spaCy\n",
    "- This ensures more **relevant and human-readable tags**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a099f08-82a8-4d3f-8050-d86b20e37c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Tags (TF-IDF + POS): ['accuracy', 'ai', 'allocation', 'artificial', 'bias', 'care', 'compliance']\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords and spacy model\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Basic cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Enhancing TF-IDF with POS filtering\n",
    "def extract_clean_tags(text, top_n=7):\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform([text])\n",
    "    words = tfidf.get_feature_names_out()\n",
    "    scores = tfidf.idf_\n",
    "    word_score_pairs = dict(zip(words, scores))\n",
    "\n",
    "    # Using spaCy to keep only nouns & proper nouns\n",
    "    doc = nlp(text)\n",
    "    noun_tokens = {token.lemma_.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop}\n",
    "\n",
    "    filtered = {word: score for word, score in word_score_pairs.items() if word in noun_tokens}\n",
    "    sorted_filtered = sorted(filtered.items(), key=lambda x: x[1])\n",
    "    return [word for word, score in sorted_filtered[:top_n]]\n",
    "\n",
    "# Apply\n",
    "cleaned_text = clean_text(sample_text)\n",
    "tags = extract_clean_tags(sample_text)\n",
    "print(\"Top Tags (TF-IDF + POS):\", tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d131d2-35d5-4b3e-a8c6-1d94313ebf64",
   "metadata": {},
   "source": [
    "## Discover Topics using LDA\n",
    "\n",
    "I have used Latent Dirichlet Allocation (LDA) from Gensim to detect **latent topics** from the article. \n",
    "\n",
    "Each **paragraph is treated as a separate document**, and words that frequently co-occur are grouped into topics.\n",
    "\n",
    "**Why LDA?**\n",
    "- Unsupervised learning algorithm\n",
    "- Helps summarize large corpora into 2–5 key themes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32200873-da6b-4ccd-ac44-2808609d7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dominant Topics:\n",
      "Topic 1: 0.034*\"healthcare\" + 0.024*\"data\" + 0.024*\"learning\" + 0.024*\"integration\" + 0.024*\"challenges\" + 0.014*\"diagnostics\"\n",
      "Topic 2: 0.020*\"models\" + 0.020*\"patient\" + 0.020*\"using\" + 0.012*\"also\" + 0.012*\"integrated\" + 0.012*\"care\"\n",
      "Topic 3: 0.023*\"data\" + 0.023*\"medical\" + 0.023*\"human\" + 0.013*\"need\" + 0.013*\"researchers\" + 0.013*\"processing\"\n"
     ]
    }
   ],
   "source": [
    "# Split into paragraphs for LDA\n",
    "paragraphs = [p.strip() for p in sample_text.split(\"\\n\") if len(p.strip()) > 50]\n",
    "tokenized_paragraphs = [clean_text(p).split() for p in paragraphs]\n",
    "\n",
    "# Gensim LDA setup\n",
    "dictionary = corpora.Dictionary(tokenized_paragraphs)\n",
    "bow_corpus = [dictionary.doc2bow(p) for p in tokenized_paragraphs]\n",
    "\n",
    "# Train LDA\n",
    "lda_model = LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=3, passes=20, random_state=42)\n",
    "topics = lda_model.print_topics(num_words=6)\n",
    "\n",
    "print(\"\\nDominant Topics:\")\n",
    "for i, topic in topics:\n",
    "    print(f\"Topic {i+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761f33d-1365-4da2-8ebe-abc13192b3ae",
   "metadata": {},
   "source": [
    "## Final Output Summary\n",
    "\n",
    "Here, printing:\n",
    "- The top tags detected via TF-IDF + POS\n",
    "- The 3 main topics uncovered using LDA\n",
    "\n",
    "These outputs provide semantic insight into the key terms and themes from the input article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f1751d0-62a8-49a4-b4c3-a0d412acb779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Summary:\n",
      "Top Tags (TF-IDF + POS): ['accuracy', 'ai', 'allocation', 'artificial', 'bias', 'care', 'compliance']\n",
      "\n",
      "LDA Topics:\n",
      "Topic 1: 0.034*\"healthcare\" + 0.024*\"data\" + 0.024*\"learning\" + 0.024*\"integration\" + 0.024*\"challenges\" + 0.014*\"diagnostics\"\n",
      "Topic 2: 0.020*\"models\" + 0.020*\"patient\" + 0.020*\"using\" + 0.012*\"also\" + 0.012*\"integrated\" + 0.012*\"care\"\n",
      "Topic 3: 0.023*\"data\" + 0.023*\"medical\" + 0.023*\"human\" + 0.013*\"need\" + 0.013*\"researchers\" + 0.013*\"processing\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal Summary:\")\n",
    "print(\"Top Tags (TF-IDF + POS):\", tags)\n",
    "\n",
    "print(\"\\nLDA Topics:\")\n",
    "for i, topic in topics:\n",
    "    print(f\"Topic {i+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b34f03-d115-4bd9-9251-f31c99b84710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
